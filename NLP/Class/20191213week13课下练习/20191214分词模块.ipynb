{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jieba分词\n",
    "\n",
    "- https://github.com/fxsjy/jieba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三种分词模式\n",
    "- 精确模式：将句子最精确地切开，适用于文本分析\n",
    "- 全模式：把句子中所有可以成词的词语都扫描出来，速度非常快，但是不能解决起义问题\n",
    "- 搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
      "Default Mode: 我/ 来到/ 北京/ 清华大学\n",
      "他, 来到, 了, 网易, 杭研, 大厦\n",
      "小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式\n",
    "\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\")  # 默认是精确模式\n",
    "print(\", \".join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")  # 搜索引擎模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入自定义词典\n",
    "\n",
    "- 开发者可以载入自己自定义的词典，以便包含jieba词库里面没有的词语。（自行添加可以确保更高的正确率）\n",
    "- 用法： jieba.load_userdict(file_name) # file_name 为文件类对象或自定义词典的路径\n",
    "- 词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。\n",
    "- 词频省略时使用自动计算的能保证分出该词的词频。\n",
    "\n",
    "- 可以调整自定义词典，从而动态修改词典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果/放到/post/中将/出错/。\n"
     ]
    }
   ],
   "source": [
    "print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.suggest_freq(('中', '将'), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果/放到/post/中/将/出错/。\n"
     ]
    }
   ],
   "source": [
    "print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「/台/中/」/正确/应该/不会/被/切开\n"
     ]
    }
   ],
   "source": [
    "print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.suggest_freq('台中', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「/台中/」/正确/应该/不会/被/切开\n"
     ]
    }
   ],
   "source": [
    "print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关键词提取\n",
    "\n",
    "- import jieba.analyse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文分词+词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我 爱 自然 自然语言 语言 处理\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"我爱自然语言处理\", cut_all=True)         \n",
    "print(\"Full Mode: \" + \" \".join(seg_list)) # 全模式  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "import jieba.posseg\n",
    " \n",
    "def dosegment_all(sentence):\n",
    "    '''\n",
    "    带词性标注，对句子进行分词，不排除停词等\n",
    "    :param sentence:输入字符\n",
    "    :return:\n",
    "    '''\n",
    "    sentence_seged = jieba.posseg.cut(sentence.strip())\n",
    "    outstr = ''\n",
    "    for x in sentence_seged:\n",
    "        outstr+=\"{}/{},\".format(x.word,x.flag)\n",
    "    #上面的for循环可以用python递推式构造生成器完成\n",
    "    # outstr = \",\".join([(\"%s/%s\" %(x.word,x.flag)) for x in sentence_seged])\n",
    "    return outstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "爱 v\n",
      "北京 ns\n",
      "天安门 ns\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words =pseg.cut(\"我爱北京天安门\")\n",
    "for w in words:\n",
    "    print(w.word,w.flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnowNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文分词+词性标注\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "杭州 西湖 风景 很 好 ， 是 旅游 胜地 ！\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hang',\n",
       " 'zhou',\n",
       " 'xi',\n",
       " 'hu',\n",
       " 'feng',\n",
       " 'jing',\n",
       " 'hen',\n",
       " 'hao',\n",
       " '，',\n",
       " 'shi',\n",
       " 'lv',\n",
       " 'you',\n",
       " 'sheng',\n",
       " 'di',\n",
       " '！']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snownlp import SnowNLP\n",
    "s = SnowNLP(\"杭州西湖风景很好，是旅游胜地！\")\n",
    "print(' '.join(s.words))   \n",
    "s.pinyin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'「繁体字」「繁体中文」的叫法在台湾亦很常见。'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 繁体转为简体\n",
    "s = SnowNLP(u'「繁體字」「繁體中文」的叫法在臺灣亦很常見。')\n",
    "s.han"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['因而它是计算机科学的一部分',\n",
       " '自然语言处理是计算机科学领域与人工智能领域中的一个重要方向',\n",
       " '自然语言处理是一门融语言学、计算机科学、数学于一体的科学',\n",
       " '所以它与语言学的研究有着密切的联系',\n",
       " '这一领域的研究将涉及自然语言']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动摘要\n",
    "text = u'''自然语言处理是计算机科学领域与人工智能领域中的\n",
    "一个重要方向。它研究能实现人与计算机之间用自然语言进行有效通信\n",
    "的各种理论和方法。自然语言处理是一门融语言学、计算机科学、\n",
    "数学于一体的科学。因此，这一领域的研究将涉及自然语言，\n",
    "即人们日常使用的语言，所以它与语言学的研究有着密切的联系，\n",
    "但又有重要的区别。自然语言处理并不是一般地研究自然语言，\n",
    "而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。\n",
    "因而它是计算机科学的一部分。\n",
    "'''\n",
    "s = SnowNLP(text)\n",
    "s.summary(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.45263029953948547, -0.45263029953948547, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算BM2.5 相似性（计算相似性）\n",
    "s = SnowNLP([[u'这篇', u'文章',u'非常好'],\n",
    "             [u'那篇', u'文章',u'一般般'],\n",
    "             [u'这个']])\n",
    "#s.tf\n",
    "s.idf\n",
    "s.sim([u'这篇', u'文章',u'非常好'])# [0.3756070762985226, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本相似度-bm25算法原理以及实现\n",
    "\n",
    "- 搜索相关性评分\n",
    "- 对Query进行语素解析，生成语素qi；然后，对于每个搜索结果D，计算每个语素qi与D的相关性得分，最后，将qi相对于D的相关性得分进行加权求和，从而得到Query与D的相关性得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-8b2f4be1ed04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0minput_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0moutput_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mread_and_analysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-8b2f4be1ed04>\u001b[0m in \u001b[0;36mread_and_analysis\u001b[1;34m(input_file, output_file)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_and_analysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mfw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "#coding:UTF-8\n",
    "import sys\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "def read_and_analysis(input_file, output_file):\n",
    "    f = open(input_file)\n",
    "    fw = open(output_file, \"w\")\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        lines = line.strip().split(\"\\t\")\n",
    "        if len(lines) < 2:\n",
    "            continue\n",
    "\n",
    "        s = SnowNLP(lines[1].decode('utf-8'))\n",
    "        # s.words 查询分词结果\n",
    "        seg_words = \"\"\n",
    "        for x in s.words:\n",
    "            seg_words += \"_\"\n",
    "            seg_words += x\n",
    "        # s.sentiments 查询最终的情感分析的得分\n",
    "        fw.write(lines[0] + \"\\t\" + lines[1] + \"\\t\" + seg_words.encode('utf-8') + \"\\t\" + str(s.sentiments) + \"\\n\")\n",
    "    fw.close()\n",
    "    f.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = sys.argv[1]\n",
    "    output_file = sys.argv[2]\n",
    "    read_and_analysis(input_file, output_file)\n",
    "    \n",
    "# 上述代码会从文件中读取每一行的文本，并对其进行情感分析并输出最终的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 情感分析源码分析：https://blog.csdn.net/google19890102/article/details/80091502"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句法分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 暂时没找到！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGram语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('add',)\n",
      "('domain',)\n",
      "('with',)\n",
      "('authentication',)\n",
      "('for',)\n",
      "('conference',)\n",
      "('focus',)\n",
      "('user',)\n",
      "('add', 'domain')\n",
      "('domain', 'with')\n",
      "('with', 'authentication')\n",
      "('authentication', 'for')\n",
      "('for', 'conference')\n",
      "('conference', 'focus')\n",
      "('focus', 'user')\n",
      "++++++++++++++\n",
      "('add', 'domain', 'with')\n",
      "('domain', 'with', 'authentication')\n",
      "('with', 'authentication', 'for')\n",
      "('authentication', 'for', 'conference')\n",
      "('for', 'conference', 'focus')\n",
      "('conference', 'focus', 'user')\n",
      "++++++++++++++\n",
      "('add', 'domain', 'with', 'authentication')\n",
      "('domain', 'with', 'authentication', 'for')\n",
      "('with', 'authentication', 'for', 'conference')\n",
      "('authentication', 'for', 'conference', 'focus')\n",
      "('for', 'conference', 'focus', 'user')\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "a = \"add domain with authentication for conference focus user\".split(' ')\n",
    "unigrams = ngrams(a,1)\n",
    "for i in unigrams:\n",
    "    print(i)\n",
    " \n",
    "bigrams = ngrams(a,2)\n",
    "for i in bigrams:\n",
    "    print(i)\n",
    "\n",
    "print('++++++++++++++')\n",
    "\n",
    "trigrams = ngrams(a,3)\n",
    "for i in trigrams:\n",
    "    print(i)\n",
    "    \n",
    "print('++++++++++++++')\n",
    "\n",
    "fourgrams = ngrams(a,4)\n",
    "for i in fourgrams:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK使用\n",
    "\n",
    "- 分词\n",
    "- 分句\n",
    "- 词干提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"this's a sent tokenize test.\", 'this is sent two.', 'is this sent three?', 'sent 4 is cool!', \"Now it's your turn.\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "# NLTK 分词\n",
    "word_tokenize(\"this's a test\")\n",
    "# NLTK 分句\n",
    "text = \"this's a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it's your turn.\"\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "print(sent_tokenize_list)\n",
    "\n",
    "text = nltk.word_tokenize(\"Dive into NLTK: Part-of-speech tagging and POS Tagger\")\n",
    "nltk.pos_tag(text)\n",
    "\n",
    "# 词干提取\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "porter_stemmer.stem('maximum')\n",
    "lancaster_stemmer.stem('maximum')\n",
    "\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize('dogs')\n",
    "wordnet_lemmatizer.lemmatize('aardwolves')\n",
    "\n",
    "wordnet_lemmatizer.lemmatize('is', pos='v')\n",
    "wordnet_lemmatizer.lemmatize('are', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', \"'s\", 'a', 'test']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK 分词\n",
    "word_tokenize(\"this's a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"this's a sent tokenize test.\", 'this is sent two.', 'is this sent three?', 'sent 4 is cool!', \"Now it's your turn.\"]\n"
     ]
    }
   ],
   "source": [
    "# NLTK 分句\n",
    "text = \"this's a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it's your turn.\"\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "print(sent_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dive', 'NNP'),\n",
       " ('into', 'IN'),\n",
       " ('NLTK', 'NNP'),\n",
       " (':', ':'),\n",
       " ('Part-of-speech', 'JJ'),\n",
       " ('tagging', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('POS', 'NNP'),\n",
       " ('Tagger', 'NNP')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"Dive into NLTK: Part-of-speech tagging and POS Tagger\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maxim'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "porter_stemmer.stem('maximum')\n",
    "lancaster_stemmer.stem('maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aardwolf'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize('dogs')\n",
    "wordnet_lemmatizer.lemmatize('aardwolves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('is', pos='v')\n",
    "wordnet_lemmatizer.lemmatize('are', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('is', pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词法分析\n",
    "\n",
    "- 将句子转换成词序列并标记句子中的词的词性等\n",
    "- 不同语言词法分析任务有所不同\n",
    "\n",
    "### 英语\n",
    "\n",
    "- 用空格隔开，不需要分词\n",
    "- 用词的形态变化表示语法关系\n",
    "- 英文词法分析（曲折变化） i. 词的识别、词形还原 ii. 未登录词处理 iii.词性标注\n",
    "\n",
    "### 汉语\n",
    "\n",
    "- 词与词紧密相边，没有明显分界标志\n",
    "- 词形变化少，靠词序或虚词来表示语法关系\n",
    "- 中文词法分析 i. 分词 ii.未登录词识别 iii.词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['在', '尼', '比鲁', '星球', '探查', '期间', '，', '企业', '号', '舰长', '柯克', '为', '营救', '史', '波克', '采取', '了', '胆大妄为', '的', '举动', '，', '几乎', '危及', '全', '舰队', '员', '的', '生命', '，', '他', '也', '为', '此', '付出', '代价', '。']\n",
      "[('在', 'p'), ('尼', 'j'), ('比鲁', 'j'), ('星球', 'n'), ('探查', 'v'), ('期间', 'f'), ('，', 'w'), ('企业', 'n'), ('号', 'n'), ('舰长', 'n'), ('柯克', 'ad'), ('为', 'v'), ('营救', 'v'), ('史', 'Ng'), ('波克', 'o'), ('采取', 'v'), ('了', 'u'), ('胆大妄为', 'i'), ('的', 'u'), ('举动', 'n'), ('，', 'w'), ('几乎', 'd'), ('危及', 'v'), ('全', 'a'), ('舰队', 'n'), ('员', 'Ng'), ('的', 'u'), ('生命', 'n'), ('，', 'w'), ('他', 'r'), ('也', 'd'), ('为', 'p'), ('此', 'r'), ('付出', 'v'), ('代价', 'n'), ('。', 'w')]\n"
     ]
    }
   ],
   "source": [
    "# 使用snownlp进行词法分析\n",
    "from snownlp import SnowNLP\n",
    "text = \"在尼比鲁星球探查期间，企业号舰长柯克为营救史波克采取了胆大妄为的举动，几乎危及全舰队员的生命，他也为此付出代价。\"\n",
    "s = SnowNLP(text)\n",
    "print(s.words)\n",
    "print([t for t in s.tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk.tree import ParentedTree as PT\n",
    "from nltk.treeprettyprinter import TreePrettyPrinter\n",
    "\n",
    "corenlp = StanfordCoreNLP('http://localhost',port=2002)\n",
    "text=\"Once again, Coca-Cola, Nestlé, and PepsiCo are the world's worst plastic pollution contributors, according to a recent global audit.\"\n",
    "tstr = corenlp.parse(text)\n",
    "pt = PT.fromstring(tstr)\n",
    "tstr_pretty = TreePrettyPrinter(pt).text()\n",
    "print(tstr_pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
