{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['在',\n",
       " '尼',\n",
       " '比鲁',\n",
       " '星球',\n",
       " '探查',\n",
       " '期间',\n",
       " '，',\n",
       " '企业',\n",
       " '号',\n",
       " '舰长',\n",
       " '柯克',\n",
       " '为',\n",
       " '营救',\n",
       " '史',\n",
       " '波克',\n",
       " '采取',\n",
       " '了',\n",
       " '胆大妄为',\n",
       " '的',\n",
       " '举动',\n",
       " '，',\n",
       " '几乎',\n",
       " '危及',\n",
       " '全',\n",
       " '舰队',\n",
       " '员',\n",
       " '的',\n",
       " '生命',\n",
       " '，',\n",
       " '他',\n",
       " '也',\n",
       " '为',\n",
       " '此',\n",
       " '付出',\n",
       " '代价',\n",
       " '。']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"在尼比鲁星球探查期间，企业号舰长柯克为营救史波克采取了胆大妄为的举动，几乎危及全舰队员的生命，他也为此付出代价。\"\n",
    "\n",
    "# snownlp\n",
    "from snownlp import SnowNLP\n",
    "doc = SnowNLP(text)\n",
    "doc.words\n",
    "\n",
    "# 基本正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "snow_text = doc.words\n",
    "print(len(snow_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# jieba\n",
    "import jieba\n",
    "words = jieba.cut(text)\n",
    "# [w for w in words]\n",
    "\n",
    "# 基本正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "jieba_text = []\n",
    "for w in words:\n",
    "    jieba_text.append(w)\n",
    "print(len(jieba_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['在',\n",
       " '尼比鲁星球',\n",
       " '探查',\n",
       " '期间',\n",
       " '，',\n",
       " '企业号',\n",
       " '舰长',\n",
       " '柯克',\n",
       " '为',\n",
       " '营救',\n",
       " '史波克',\n",
       " '采取',\n",
       " '了',\n",
       " '胆大妄为',\n",
       " '的',\n",
       " '举动',\n",
       " '，',\n",
       " '几乎',\n",
       " '危及',\n",
       " '全舰',\n",
       " '队员',\n",
       " '的',\n",
       " '生命',\n",
       " '，',\n",
       " '他',\n",
       " '也',\n",
       " '为此',\n",
       " '付出',\n",
       " '代价',\n",
       " '。']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_text = []\n",
    "people_text = '在 尼比鲁星球 探查 期间 ， 企业号 舰长 柯克 为 营救 史波克 采取 了 胆大妄为 的 举动 ， 几乎 危及 全舰 队员 的 生命 ， 他 也 为此 付出 代价 。'\n",
    "\n",
    "people_text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_text = people_text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本正确的结果\n",
    "\n",
    "# 在 尼比鲁星球 探查 期间 ，企业号 舰长 柯克 为 营救 史波克 采取 了 胆大妄为 的 举动 ， 几乎 危及 全舰 队员 的 生命 ， 他 也 为此 付出 代价 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准确率\n",
    "def count_precision(raw_text,right_text):\n",
    "    i = 0\n",
    "    for words in raw_text:\n",
    "        if words in right_text:\n",
    "            i = i+1\n",
    "    p = i/len(raw_text)\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jieba 0.7741935483870968\n",
      "snownlp 0.6944444444444444\n"
     ]
    }
   ],
   "source": [
    "# p\n",
    "print('jieba',count_precision(jieba_text,right_text))\n",
    "print('snownlp',count_precision(snow_text,right_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 召回率recall\n",
    "\n",
    "def count_recall(raw_text,right_text):\n",
    "    i = 0\n",
    "    for words in raw_text:\n",
    "        if words in right_text:\n",
    "            i = i+1\n",
    "    r = i/len(right_text)\n",
    "    return(r) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jieba 0.8\n",
      "snownlp 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# r\n",
    "print('jieba',count_recall(jieba_text,right_text))\n",
    "print('snownlp',count_recall(snow_text,right_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_measure(raw_text,right_text):\n",
    "    i = 0\n",
    "    for words in raw_text:\n",
    "        if words in right_text:\n",
    "            i = i+1\n",
    "    p = i/len(raw_text)\n",
    "    r = i/len(right_text)\n",
    "    f = (2*p*r)/(p+r)\n",
    "    return(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jieba 0.7868852459016393\n",
      "snownlp 0.7575757575757577\n"
     ]
    }
   ],
   "source": [
    "# f\n",
    "print('jieba',f_measure(jieba_text,right_text))\n",
    "print('snownlp',f_measure(snow_text,right_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词法分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 7,\n",
      " 'b': 1,\n",
      " 'c': 4,\n",
      " 'd': 4,\n",
      " 'f': 1,\n",
      " 'k': 1,\n",
      " 'l': 1,\n",
      " 'm': 4,\n",
      " 'n': 12,\n",
      " 'nx': 1,\n",
      " 'p': 2,\n",
      " 'q': 2,\n",
      " 'r': 2,\n",
      " 'u': 13,\n",
      " 'v': 14,\n",
      " 'vd': 1,\n",
      " 'vn': 3,\n",
      " 'w': 8}\n"
     ]
    }
   ],
   "source": [
    "# 统计单词的词性分布\n",
    "\n",
    "from snownlp import SnowNLP\n",
    "from pprint import pprint\n",
    "text = \"2019年即将结束，与之相伴的是健康保健行业中一些有趣的新趋势。这是一个不断发展的领域，随着新技术和信息的广大普及，新出现的趋势受到了人们的关注。2019年，我们看到越来越多的人选择了新疗法，包括大麻二酚产品的增加，以及致力于改善健康的应用程序和技术的持续增长。\"\n",
    "s = SnowNLP(text)\n",
    "tags = s.tags# 进行词性标注\n",
    "distr = {}\n",
    "for t in tags:\n",
    "    if not t[1] in distr.keys():\n",
    "        distr[t[1]]=1\n",
    "    else:\n",
    "        distr[t[1]]+=1\n",
    "pprint(distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "snownlp.SnowNLP"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SnowNLP in module snownlp object:\n",
      "\n",
      "class SnowNLP(builtins.object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, doc)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  keywords(self, limit=5, merge=False)\n",
      " |  \n",
      " |  sim(self, doc)\n",
      " |  \n",
      " |  summary(self, limit=5)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  han\n",
      " |  \n",
      " |  idf\n",
      " |  \n",
      " |  pinyin\n",
      " |  \n",
      " |  sentences\n",
      " |  \n",
      " |  sentiments\n",
      " |  \n",
      " |  tags\n",
      " |  \n",
      " |  tf\n",
      " |  \n",
      " |  words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先切词\n",
    "# 进行词性标记（英文中标记词性，进行词性还原【名词单复数、所有格、专有名词、代词、形容词的词性、动词数、时态变化】）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 CD\n",
      "is VBZ\n",
      "coming VBG\n",
      "to IN\n",
      "a DT\n",
      "close NN\n",
      ", ,\n",
      "and CC\n",
      "with IN\n",
      "it PRP\n",
      ", ,\n",
      "we PRP\n",
      "'ve VB\n",
      "seen VBN\n",
      "some DT\n",
      "interesting JJ\n",
      "trends NNS\n",
      "that WDT\n",
      "come VBP\n",
      "and CC\n",
      "go VB\n",
      "in IN\n",
      "the DT\n",
      "health NN\n",
      "and CC\n",
      "wellness NN\n",
      "industry NN\n",
      ". .\n",
      "2019 CD\n",
      "is VBZ\n",
      "coming VBG\n",
      "to IN\n",
      "a DT\n",
      "close NN\n",
      ", ,\n",
      "and CC\n",
      "with IN\n",
      "it PRP\n",
      ", ,\n",
      "we PRP\n",
      "'ve VB\n",
      "seen VBN\n",
      "some DT\n",
      "interesting JJ\n",
      "trends NNS\n",
      "that WDT\n",
      "come VBP\n",
      "and CC\n",
      "go VB\n",
      "in IN\n",
      "the DT\n",
      "health NN\n",
      "and CC\n",
      "wellness NN\n",
      "industry NN\n",
      ". .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[a close,\n",
       " it,\n",
       " we,\n",
       " some interesting trends,\n",
       " the health and wellness industry,\n",
       " a constantly evolving area,\n",
       " areas,\n",
       " interest,\n",
       " new trends,\n",
       " new technology,\n",
       " information,\n",
       " the broader public,\n",
       " we,\n",
       " an interesting shift,\n",
       " new therapies,\n",
       " an increase,\n",
       " CBD-based products,\n",
       " a continued rise,\n",
       " the number,\n",
       " apps,\n",
       " technology,\n",
       " well-being,\n",
       " health]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text=\"2019 is coming to a close, and with it, we've seen some interesting trends that come and go in the health and wellness industry. This is a constantly evolving area where areas of interest come and go with new trends appearing as new technology and information are made accessible to the broader public. In 2019, we saw an interesting shift toward new therapies, including an increase in CBD-based products, as well as a continued rise in the number of apps and technology to improve well-being and health.\"\n",
    "doc = nlp(text)\n",
    "# 分句\n",
    "list(doc.sents)\n",
    "# 词性import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text=\"2019 is coming to a close, and with it, we've seen some interesting trends that come and go in the health and wellness industry. This is a constantly evolving area where areas of interest come and go with new trends appearing as new technology and information are made accessible to the broader public. In 2019, we saw an interesting shift toward new therapies, including an increase in CBD-based products, as well as a continued rise in the number of apps and technology to improve well-being and health.\"\n",
    "doc = nlp(text)\n",
    "# 分句\n",
    "list(doc.sents)\n",
    "# 词性标注\n",
    "for w in list(doc.sents)[0]:print(w,w.tag_)\n",
    "all_tags = {w.pos: w.pos_ for w in doc}\n",
    "# 命名实体检测\n",
    "[ (w,w.label_)  for w in doc.ents]\n",
    "# 名词性短语\n",
    "[np for np in doc.noun_chunks]#标注\n",
    "for w in list(doc.sents)[0]:print(w,w.tag_)\n",
    "all_tags = {w.pos: w.pos_ for w in doc}\n",
    "# 命名实体检测\n",
    "[ (w,w.label_)  for w in doc.ents]\n",
    "# 名词性短语\n",
    "[np for np in doc.noun_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 文本特征抽取（lexical...）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
