{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该英文文本的句子数为： 2\n",
      "该英文文本单词中的平均句长为： 3.5\n",
      "该英文文本单词中的平均句长的标准偏差为： 0.707106781187\n",
      "该中文文本的句子数为： 4\n",
      "该中文文本句子的平均句长为： 7.5\n",
      "该中文文本句子的平均句长的标准偏差为： 1.29099444874\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import jieba.posseg as pseg\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def ReadContent(fname):\n",
    "    \"\"\"\n",
    "    读取文件内容，返回文件内容\n",
    "    \"\"\"\n",
    "    with open(fname, 'r', encoding = 'utf-8')as file_object:\n",
    "        content = file_object.read()\n",
    "    return content\n",
    "\n",
    "def WriteContent(file_name, content):\n",
    "    '''\n",
    "    将内容content写入名为file_name的文件\n",
    "    '''\n",
    "    with open(file_name, \"w\",encoding='utf-8') as f:\n",
    "        json.dump(content, f,ensure_ascii=False,indent = 4)\n",
    "    return ''\n",
    "\n",
    "#--------------------------英文-----------------------\n",
    "\n",
    "#计算英文文章句子数\n",
    "def DESSC_en(content):\n",
    "    \n",
    "    text = sent_tokenize(content)\n",
    "    s=0\n",
    "    for j in text:\n",
    "        s+=1#句子数\n",
    "         \n",
    "    return s,text#句子数,处理内容\n",
    "\n",
    "\n",
    "#计算英文句子平均长度\n",
    "def DESSL_en(content):  \n",
    "    s= DESSC_en(content)[0]#句子数\n",
    "    text = DESSC_en(content)[1]#处理后的内容\n",
    "    length=[]\n",
    "    for i in text:\n",
    "        length.append(len(word_tokenize(i)))#计算每个句子的长度，放入length里\n",
    "    \n",
    "    sum_len=0\n",
    "    for j in length:\n",
    "        sum_len += j#把length list里的数加到一起\n",
    "        \n",
    "    average_len = sum_len/s   #计算平均值\n",
    "    return average_len, length#平均句长，文章句长列表\n",
    "\n",
    "\n",
    "#计算英文句长标准差\n",
    "def DESSLd_en(content):\n",
    "    length = DESSL_en(content)[1]#句长列表\n",
    "    my_num = np.std(length, ddof = 1)    #需要得到样本标准差\n",
    "    return my_num\n",
    "\n",
    "#--------------------中文---------------------\n",
    "def DESSC_zh(content):\n",
    "    \"\"\"\n",
    "    计算中文文章句子数量\n",
    "    \"\"\"\n",
    "    content = re.split('(。|！|\\!|\\.|？|\\?)',content)\n",
    "    lengths = []\n",
    "    s=0\n",
    "    for i in range(int(len(content)/2)):\n",
    "        sent = content[2*i] + content[2*i+1]\n",
    "        s+=1\n",
    "        lengths.append(len(sent))\n",
    "    #print(lengths)\n",
    "    return s,lengths#句子数，长度列表\n",
    "\n",
    "\n",
    "\n",
    "def DESSL_zh(content):\n",
    "    \"\"\"\n",
    "    计算中文句子的平均长度\n",
    "    \"\"\"\n",
    "    s= DESSC_zh(content)[0]#句子数\n",
    "    lengths = DESSC_zh(content)[1]#长度列表\n",
    "    sum_len = 0\n",
    "    for i in lengths:\n",
    "        sum_len +=i\n",
    "    average_len = sum_len/s\n",
    "    return average_len,lengths\n",
    "\n",
    "\n",
    "def DESSLd_zh(content):\n",
    "    \"\"\"\n",
    "    计算中文句子标准差\n",
    "    \"\"\"\n",
    "    lengths = DESSL_zh(content)[1]\n",
    "    my_num = np.std(lengths, ddof = 1)    # 需要得到样本标准差\n",
    "    return my_num\n",
    "\n",
    "    \n",
    "#------------------------------------------------------------------\n",
    "\n",
    "def description_sentence_en(fname):\n",
    "    \"\"\"\n",
    "    上述信息进行整合并输出\n",
    "    \"\"\"\n",
    "    text_en = ReadContent(fname)\n",
    "    sum_sentence = DESSC_en(text_en)[0]\n",
    "    aver_sentence = DESSL_en(text_en)[0]\n",
    "    std_sentence = DESSLd_en(text_en)\n",
    "    print('该英文文本的句子数为：', sum_sentence)\n",
    "    print('该英文文本单词中的平均句长为：', aver_sentence)\n",
    "    print('该英文文本单词中的平均句长的标准偏差为：', std_sentence)\n",
    "    \n",
    "def description_sentence_zh(fname):\n",
    "    \"\"\"\n",
    "    上述信息进行整合并输出\n",
    "    \"\"\"\n",
    "    text_zh = ReadContent(fname)\n",
    "    sum_sentence = DESSC_zh(text_zh)[0]\n",
    "    aver_sentence = DESSL_zh(text_zh)[0]\n",
    "    std_sentence = DESSLd_zh(text_zh)\n",
    "    print('该中文文本的句子数为：', sum_sentence)\n",
    "    print('该中文文本句子的平均句长为：', aver_sentence)\n",
    "    print('该中文文本句子的平均句长的标准偏差为：', std_sentence)\n",
    "\n",
    "\n",
    "# 修改文本名称即可\n",
    "description_sentence_en('2.txt')\n",
    "description_sentence_zh('1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
